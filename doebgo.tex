\documentclass[index]{subfiles}
\begin{document}
\chapter{从计算机实验设计到贝叶斯优化}
计算机实验设计\footnote{需要注意的是，本文中术语“计算机实验设计”指的是Design of Computer Experiments，即“计算机实验”的设计，并非利用计算机进行传统实验设计。}是20世纪末随着电子计算机的蓬勃发展而产生的新兴学科\cite{mckay1979}。
计算机实验设计方法和思想的进展扩展了全局最优化问题的求解思路。
本章将先分别回顾计算机实验设计和全局最优化两个学科独立发展的历史，
再着重介绍两者的有机结合——贝叶斯优化算法的思想和实现细节。

\section{计算机实验设计}
虽然计算机实验设计与传统实验设计有着明显的差异，但两者还是有一些相似之处值得研究。
为了引入计算机实验设计的概念，本节将会先介绍传统实验设计的基本方法，
再介绍无模型的计算机实验设计方法，最后介绍统计模型计算机实验设计方法及其与全局最优化问题的联系。

\subsection{传统实验设计方法的演进}
实验设计（Design of Experiment）学科有着相当悠久的历史。
为了研究不同因素对系统的影响，科学界和工业界人士通常都会对其进行实验：
人为设置这些因素为特定的值，观察系统的输出结果，再对结果进行分析。
如何选定这些值以方便分析、如何进行分析以减弱不可控因素造成的影响的学问也就构成了实验设计学科\cite{davies1954}。
进行实验的目的一般有两类：判断某些因素的影响有无/强弱，和调整因素以提高系统性能。
前者意味着实验完毕进行数据分析时会采用线性回归分析和方差分析，
而后者意味着将会采用非线性回归分析或者更为复杂的手段，并经常意味着需要迭代多次实验。
本小节将先介绍设计基本原则，再分别讨论方差分析和回归分析所对应的设计手段。

\subsubsection{设计基本原则}
实验设计领域经典教材~\inlinecite{montgomery}指出，为了对抗未知因素、提高实验结果的精度，（传统的）实验设计在进行设计时普遍遵循以下三大原则：
\begin{description}
  \item[随机化（Randomization）] 随机化可以将未知因素的影响控制在最小，以提高实验结果的可靠性。
  \item[重复实验（Replication）] 重复实验可以可以提高实验结果的精度，还可以得到实验误差的估计。
  \item[分块（Blocking）] 分块可以把不感兴趣的因素的影响和感兴趣的因素的影响抽离开，以得到正确的分析结果。
\end{description}

\subsubsection{方差分析对应的设计手段}
由于方差分析不对变量做任何距离上的假设（所有变量都是分类变量），所以枚举变量所有可能的取值，也即阶乘设计（Factorial Design）方法，得到了最为广泛的应用。
这种方法又叫正交表\cite{刘瑞江2010}：在保证平衡性（每个设计组合的观测数量相同）和正交性（任意两个设计组合在内积空间中正交）的基础上，
展开感兴趣的变量（主要作用和低阶次要作用），折叠不感兴趣的变量（高阶次要作用），得到最终的设计方案。
这种方法对于每个变量只有两个水平的情况非常合适，但在水平数量$\geq3$的情况下，正交表的计算异常困难。

\subsubsection{回归分析对应的设计手段}\label{ssec:doe-reg}
回归分析的基本假设是系统的输出$y$关于输入$x$满足这样的线性关系\cite{montgomery}：
\begin{equation}\label{equ:doe-reg}
  y = \beta_0 + \sum_{i=1}^{k} \beta_i x_i + \varepsilon
\end{equation}
特别需要注意的是，回归分析模型\cref{equ:doe-reg}中对误差项$\varepsilon$进行的假设是独立同分布假设。这种统计模型的计算非常简单，但建模能力较弱。
\Cref{ssec:doe-gp}讨论的随机过程模型\cref{equ:doe-gp}中对误差项的假设是平稳高斯假设，比本节中的独立同分布要弱得多得多，因此\cref{equ:doe-gp}可以用来建模更为复杂的响应。

\paragraph{最优设计（*-Optimal Design）}
\Cref{equ:doe-reg}中有$k+1$个待估计参数：$\beta_0,\ldots,\beta_k$。由于对这些参数没有任何先验信息，我们此时只能采用最小二乘原则对其进行估计\cite{aguiar1995}：
\begin{equation}\label{equ:doe-reg-sol}
  \hat{\beta} = \left(X^T X\right)^{-1} X^T y
\end{equation}
其中$N\times k$矩阵$X$的每一行表示一组实验设计。

为了提高估计精度，正定矩阵$X^T X$应越“大”越好\cite{triefenbach2008}。
对于矩阵的“大小”没有一个绝对的标准，不同的标准会推导出不同的设计。最常用的标准包括：
\begin{description}
  \item[D最优] $X^T X$的行列式越大越好；
  \item[A最优] $\left(X^T X\right)^{-1}$的迹越小越好；
  \item[E最优] $X^T X$最小特征值越大越好；
  \item[T最优] $X^T X$的迹越大越好；
\end{description}
更多标准（C、G、I、V最优等）可以参见文献\inlinecite{pukelsheim1993}。

\paragraph{响应曲面方法（RSM, Response Surface Methodology）}
迭代设计实验、进行实验、利用二次回归模型分析结果，以达到优化系统性能的方法称为RSM。
与二次回归模型相匹配的实验设计方法包括CCD（Central Composite Design）和BBD（Box-Behnken Design）两种。
它们都是针对二次回归模型特殊设计的方案：在中心位置进行多次重复采样，而在边沿位置只进行一次采样，用来提高回归精度\cite{montgomery}。

\subsection{计算机实验设计的诞生}
计算机模型和实验源于对复杂自然现象的简化和模拟：通过建立计算机模型并求解，科学家可以避免进行复杂昂贵并且费时间的物理实验。
由于计算机模型具有确定性、没有噪声因素等显著特点\footnote{大部分对计算机模型的讨论都限于讨论确定性（deterministic）计算机模型，也即同一模型接受相同输入所得到的输出应该没有偏差且严格相同。本文也将遵循这一约定。}，在设计计算机实验时的考虑将会和设计传统物理实验存在显著区别\cite{sacks1989}。
传统实验设计的原则对于计算机实验设计来说反而成了累赘：不存在未知因素，多次实验只是浪费时间，不感兴趣变量也可以控制给予完全一样输入。
相较之下，计算机实验设计还带来了新的挑战：很多程序的输入可以连续取值，而非只有正负两个水平。
这导致了传统实验设计方法中最常用的阶乘设计方法完全无法使用，必须寻求新的设计方法。
本小节借鉴文献~\inlinecite{pronzato2012}将计算机实验设计分为空间填充、无模型、有模型三大类的思路，
重点介绍使用最为广泛的空间填充方法，对有模型方法作简要介绍，而把有模型方法单独放在下一小节（\cref{sseu:doe-gp}）中介绍。

值得一提的是，完全随机抽样（也即均匀分布取点）也是一种实验设计方法，虽然在计算机实验设计的文献中大多将其忽略了。
在\cref{ssec:bgo-init}中，这种方法将会与其他更“专业”的方法一同参与比较。

\subsubsection{基于空间填充的计算机实验设计}
关于这一系列问题研究最早可以追溯到1979年。
文献~\inlinecite{mckay1979}比较了两种传统实验设计方法——完全随机抽样和分层抽样，并提出了一种新的方法——Latin Hypercube抽样（简称LHS）。
为了在每个输入变量维度上都服从均匀分布，LHS方法先将每个输入变量的范围等分成$N$份，再从每份中均匀抽取一点。
最后，随机打乱$N$组$k$维数据点，实现整个样本空间上的均匀分布。
需要注意的是，每个维度上的分布均匀性是严格的，但样本空间上的均匀分布却是随机的，有一定概率会得到比较差的结果（比如排成一列的情况\cite{pronzato2012}）。
关于Lh的改进包括文献~\inlinecite{tang1993,leary2003}提出的利用正交表思想结构来进行的改进，可谓是将传统实验设计与计算机实验设计进行了有机结合。

除此之外，从空间填充入手进行实验设计的方法还有minimax和maximin方法\cite{johnson1990}。
Minimax方法（简称mM）计算样本空间上的每个点到最近的实验点的距离，并尝试最小化这些距离的最大值；
Maximin方法（简称Mm）计算所有实验点两两之间的距离，并尝试最大化这些距离的最小值。
这两种方法虽然表示起来很简单，但实际计算起来非常复杂，尤其是mM，因为其需要计算遍历整个样本空间。在样本空间连续的情况下，计算mM尤为困难。
文献~\inlinecite{pronzato2012}指出了mM和Mm法存在的致命缺陷：这两种方法虽然在整个样本空间上分布十分均匀，但投影到任意维度上都是非常差的实验设计。
这与Lh的特性正好相反。文献~\inlinecite{vandam2007}结合了LHS和Mm两者的优点，提出了maximinLHS方法。

文献~\inlinecite{deutsch2012}在LHS的基础上提出了LHSMDU方法，并详细比较了完全随机抽样、LHS、maximinLHS、LHSMDU等方法，最终证明LHSMDU相较于其他方法有很高的优越性。

\subsubsection{无模型计算机实验设计方法}
鉴于mM和Mm方法在本质上不可微，文献~\inlinecite{pronzato2012}研究了$L_q$松弛方法，利用松弛参数$q$进行调节，
将不可微的目标函数（包含$\max$和$\min$）用一簇可微函数来逼近。

关于无模型的计算机实验设计方法还有很多，如基于信息论的熵方法、基于分布密度函数的核方法等等\cite{pronzato2012}。由于篇幅有限，本文将不再赘述这类方法。

\subsection{统计模型的引入}\label{ssec:doe-gp}
文献~\inlinecite{sacks1989}在详细分析总结了实际计算机模型和实验的基础上，创造性地提出了使用随机过程来对计算机模型二次建模的思路，并将其用在实验设计上。
本小节将先详细介绍该方法所使用的随机过程模型——Kriging模型，再介绍如何利用该模型进行实验设计。

\subsubsection{随机过程与Kriging模型}\label{sssec:doe-kriging}
Kriging模型的基本假设是将关于自变量$x$的确定性实值函数$y(x)$视作关于$x$的随机过程$Y(x)$的一个实现。
在计算机实验设计领域，最常用的随机过程是（带偏置的）平稳高斯随机过程，即：
\begin{equation}\label{equ:doe-kriging}
  Y(x) = \beta_0 + Z(x)
\end{equation}
其中$\beta_0$是参数（需要估计），$Z(x)$是高斯随机过程：
\begin{align}\label{equ:doe-stationary}
  \E{Z(x)} &= 0 \\
  \Cov{Z(w)}{Z(x)} &= \sigma^2 R(w, x) = \sigma^2 R(x, w)
\end{align}
在实际情况下，一般还会把假设加强到$\sigma^2 R(w, x) = \sigma^2 R(w - x)$，也即$Z(x)$是平稳高斯随机过程。
在\cref{sssec:doe-k-o}中将会用到这一假设，而在此处先按一般情况进行考虑。

\paragraph{$R$已知，对待估计参数进行估计}
假使$R(\cdot, \cdot)$已知，且在$x_1,\ldots,x_n$处已经完成了$n$组实验，得到实验结果$y_i=y(x_i)$。
记$x_s=(x_1,\ldots,x_n)^T$，$y_s=(y_1,\ldots,y_n)^{T}$，其中s表示样本（Sample）。

从高斯随机过程的定义出发，极易验证$Y(x_i)$的联合先验分布如下：
\begin{equation}\label{equ:doe-kss-d}
  \begin{bmatrix} Y(x_1) \\ \vdots \\ Y(x_n) \end{bmatrix}
  \sim \Normal{
    \begin{bmatrix} \beta_0 \\ \vdots \\ \beta_0 \end{bmatrix}
  }{\sigma^2 K }
\end{equation}
式中
\begin{equation}\label{equ:doe-kss}
  K = \begin{bmatrix}
    R(x_1, x_1) & \ldots & R(x_1, x_n) \\
    \vdots & \ddots & \vdots \\
    R(x_n, x_1) & \ldots & R(x_n, x_n)
  \end{bmatrix}
\end{equation}
。注意到$K$每个位置的数值只与实验位置$x_s$和$R(\cdot, \cdot)$有关，与待估计参数$\beta_0,\sigma^2$、实验结果$y_s$均无关。

由于$Y(x_i)$的先验分布已知，分布中待估计参数的先验分布未知，故采用极大似然法对分布参数进行估计。限于篇幅，此处直接给出估计结果\cite{sacks1989}：
\begin{align}\label{equ:doe-par-est}
  \hat{\beta_0} &= \frac{1}{n} \sum_{i=1}^{n} y_i \\
  \hat{\sigma}^2 &= \frac{1}{n} (y_s - \beta_0)^T K_{ss}^{-1} (y_s - \beta_0)
\end{align}

\paragraph{$R$已知，对未实验位置的实验结果进行估计}
Kriging模型最关键的部分在于，对没有做实验的位置，也能通过对其他位置的实验而获得信息。
对于尚未进行实验的$x_\star$处，其所有的信息都反映在$Y(x_\star)$的后验（条件）分布上，只要求得分布便可对$y(x_\star)$进行估计。

首先，计算$Y(x_\star),Y(x_1),\ldots,Y(x_n)$的联合先验分布：
\begin{equation}\label{equ:doe-ks-d}
  \begin{bmatrix} Y(x_\star) \\ Y(x_1) \\ \vdots \\ Y(x_n) \end{bmatrix}
  \sim \Normal{
    \begin{bmatrix} \beta_0 \\ \beta_0 \\ \vdots \\ \beta_0 \end{bmatrix}
  }{\sigma^2
    \begin{bmatrix}
      1 & K_s^T \\
      K_s & K
    \end{bmatrix}
  }
\end{equation}
式中
\begin{equation}\label{equ:doe-ks}
  K_s = \left(R(x_\star, x_1), \ldots, R(x_\star, x_n)\right)^T
\end{equation}
。
通过对\cref{equ:doe-ks-d}中的协方差矩阵进行相合变换\cite{rasmussen2004}，就可以得到$Y(x_\star)$的后验分布：
\begin{equation}\label{equ:doe-y}
  Y(x_\star) \sim \Normal{\beta_0 + K_s^T K^{-1} (y_s - \beta_0 \one_n)}{\sigma^2 \left(1 - K_s^T K^{-1} K_s\right)}
\end{equation}

\paragraph{$R$未知情况的处理} 将在\cref{ssec:bgo-mdl}中介绍。

\subsubsection{基于Kriging模型的最优设计}\label{sssec:doe-k-o}
\Cref{sssec:doe-kriging}介绍了如何在给定$x_s$的情况下对整个样本空间中任意一点处的目标函数进行估计。
为了更好地进行估计，调节$x_s$的过程也就是基于Kriging模型的实验设计\cite{pronzato2012}。
不同的对估计有效性的衡量标准也就导出不同的最优实验设计：
\begin{description}
  \item[IMSE最优（Integrated Mean Squared Error-Optimal）]\cite{sacks1989} 最小化全样本空间内平方误差（也即后验分布的方差）的积分
  \item[MMSE最优（Maximum Mean Squared Error-Optimal）]\cite{sacks1989} 最小化全样本空间内平方误差（也即后验分布的方差）的最大值
\end{description}

\subsection{小结}
在三大类计算机实验设计方法中，基于Kriging模型的方法的理论基础和背景最为深厚、坚实。
不幸的是，这一类方法都无法避开对随机过程自相关函数$R$的已知假设。
克服这一难题的办法顺序实验设计（Sequential Design of Experiments）：
先用空间填充或者无模型方法设计实验，获得数据后建立Kriging模型，
再在Kriging模型基础上再次设计实验。

然而，顺序实验设计的方法与本文的根本目的——优化——格格不入：
对于优化工作流，在获得第一批实验数据以后，应该考虑的是在关键区域多做采样以获得性能提升，而不是在全样本空间内部署实验点。
为此，本文空间填充实验设计中的最新方法之一——LHSMDU方法来初始化第一批实验点。

需要特别指出的是，基于Kriging模型的实验设计并非一无是处。
一方面，Kriging模型的提出和在计算机实验上的应用对全局优化算法的发展起到了关键性的作用（\cref{ssec:go-mdl}讨论优化算法时Kriging模型会再次出现），
另一方面，对于波动较大、非常不规则的目标函数，在优化初始阶段利用Kriging模型进行顺序实验设计，可能会对避开局部最小值、提高可靠性有一定正面作用。

\section{全局最优化}
全局最优化（Global Optimization）问题指的是，对已知的确定性目标函数$f(x)$，
给定自变量$x$的范围（通常为$\mathbb{R}^n$中高维矩形），求$x^\ast$使得$f(x^\ast)=\min f(x)$。
随机化全局最优化方法（Stochastic Global Optimization）是解决全局最优化问题的最常见的一类方法，
其通过在对问题的求解过程中引入或多或少的随机性（包括算法的随机性和模型的随机性），来（在概率意义下）保证收敛到全局最优解而非局部最优解。\cite{zhigljavsky2007}
本节将会在简要介绍无统计模型的优化算法以后，着重介绍有统计模型的贝叶斯优化算法。

\subsection{无统计模型的全局优化算法}
在各种无统计模型的全局优化方法中，应用最为广泛的一类即是启发式（Heuristic）全局优化算法，如模拟退火、粒子群、遗传算法等等。
这类方法总是去试图模仿自然界中的物理现象（并在算法过程中引入随机性）。
然而，这类方法虽然在一些问题上取得了较好的效果，但理论基础并不坚实，其收敛性也大多没有数学保证，实际应用时很大程度上需要根据工程经验进行调参\cite{zhigljavsky2007}。
更重要的是，这类方法无法应用在目标函数求值十分昂贵的问题上，因为自然界中“求值”是很快的，即便“迭代”盲目一些，通过大批量地进行求值还是能找到结果；
而对于目标函数求值十分昂贵的优化问题，盲目迭代、多次求值显然是非常不合适的。

另一类无统计模型的全局优化方法是全局随机搜索（Global Random Search）。
第$j$次迭代时，先计算一个随机变量$P_j$的分布，再从中独立抽取若干个样本作为本次迭代的实验位置。
在计算$P_j$时，一般会考虑之前几次（零次、一次或者多次）迭代的结果，以更好地选择本次迭代的实验位置。
这类方法的最突出的特点在于其收敛性有严格的数学证明，而且对于目标函数的性质几乎没有任何要求\cite{zhigljavsky2007}。
不过，与之而来的问题就是，收敛速度可能比启发式算法更为缓慢，和启发式算法同样并不适合求解目标函数求值昂贵的优化问题。

\subsection{有统计模型的全局优化算法}\label{ssec:go-mdl}
% TODO
\subsection{小结}
% TODO
\section{贝叶斯优化算法的实现细节}
% TODO
\subsection{统计模型}\label{ssec:bgo-mdl}
% TODO
\subsection{收获函数}
% TODO
\subsection{约束条件}
% TODO
\subsection{初始化}\label{ssec:bgo-init}
% 在一些情况\cite{morar2017}下也有不错的表现。
% TODO
\subsection{并行性}
% TODO
\subsection{子优化问题的求解}
% TODO
\subsection{小结}
% TODO
\end{document}
